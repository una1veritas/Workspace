%利得基準について
\subsection{利得基準}
事例の集合{\it T}を部分集合${\it T_{1}},{\it T_{2}},...,{\it T_{n}}$に
分割する{\it n}値のテストがあるとする．
これらの${\it T}_{i}$がのちの部分木を考えず，
その質問だけで評価しようとすれば，{\it T}内の各${\it T_{i}}$内の
クラス分布の情報だけが利用できる．
クラス分布についての記法で，${\it freq(C_{i},S)}$は{\it S}の中で
クラス${\it C_{i}}$に属する事例数を表す．
また集合{\it S}に含まれる事例数を${\it \left|S\right|}$と表す．\\
　質問の評価基準の基礎となる情報理論的な考え方は，
「メッセージによって伝えられる情報量は，メッセージの生起確率で決まる」
という風に表せる．すなわち，情報量は，確率の値の2を底とする
対数を取り，それを-1倍にして計算され，"ビット"単位で表される．
したがって，例えば，8種類の等確率のメッセージであれば，
その各々のメッセージによって伝えられる情報量は，
$\log_{2} (\frac{1}{8})$ビット，すなわち３ビットとなる．\\
　事例の集合{\it S}からランダムに１つの事例を選び出し，
それがクラス${\it C_{j}}$に属していると知らせるとすると，
このメッセージ確率は，\\
\begin{displaymath}
\frac{freq(C_{i},S)}{|S|}
\end{displaymath}\\
であり，それが伝える情報量は，\\
\begin{displaymath}
\log_{2} \left(\frac{freq(C_{i},S)}{\left|S\right|}\right) ビット
\end{displaymath}\\
となる．このようなクラスの所属関係に関するメッセージの平均情報量を
求めるために，{\it S}内での頻度で重み付けしてクラス全体に対する
平均を求めると，\\
\begin{displaymath}
info(S)=-\sum^k_{j=1}\frac{freq(C_{j},S)}{|S|}\times\log_2\left(\frac{freq(C_{i},S)}{|S|}\right) ビット
\end{displaymath}\\
を得る．この量は集合{\it S}のエントロピーとも呼ばれる．
これを事例の集合{\it T}に適用すれば，{\it info(T)}は{\it T}内のある
1つの事例が属するクラスを同定するのに必要な情報量の平均値となる．\\
　次に，質問{\it X}の{\it n}通りの結果に合わせて
{\it T}が分割された後について，同様な評価を考えると，
クラスを同定するのに必要な情報量の期待値は，
部分集合上で荷重平均をとって，\\
\begin{displaymath}
info_{X}(T)=\sum^k_{j=1}\frac{|T_{i}|}{|T|}\times\ info(T_{i})
\end{displaymath}\\
となる．これらの差\\[-5mm]
\begin{displaymath}
gain(X)=info(T)-info_{X}(T)
\end{displaymath}\\
は，質問{\it X}で{\it T}分割することによって獲得される情報量を表す．
この情報量の利得は，質問{\it X}とクラスとの相互情報量とも呼ばれる．
これを最大にするように質問を選ぶ基準を，利得基準と呼ぶ．
